# -*- coding: utf-8 -*-
"""CarsCodingChallenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EZmsF1eldFTU_PtIw872EViz1bt--VV_
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor
from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor

import warnings
warnings.filterwarnings("ignore")

"""


4. Build a predictive model to predict the MPG based on other data. What is your optimal error metric on the test set?"""

# Reading the data
cars = pd.read_csv('cars.csv', sep=';')

print("Sample of the data")
print(cars.head())

# Dropping the first row with data_type information
print(f"First row with data_type {cars.iloc[0]}")
cars.drop(0, axis = 0, inplace = True)

# Converting the data columns to their actual data_type
cars['Car'] = cars['Car'].astype(str)
cars['MPG'] = cars['MPG'].astype(float)
cars['Cylinders'] = cars['Cylinders'].astype(int)
cars['Displacement'] = cars['Displacement'].astype(float)
cars['Horsepower'] = cars['Horsepower'].astype(float)
cars['Weight'] = cars['Weight'].astype(float)
cars['Acceleration'] = cars['Acceleration'].astype(float)
cars['Model'] = cars['Model'].astype(int)
cars['Origin'] = cars['Origin'].astype("category")

# Removing the rows where the data is incomplete
cars.drop(cars[cars.MPG == 0.0].index, axis=0, inplace=True)
cars.drop(cars[cars.Horsepower == 0.0].index, axis=0, inplace=True)
print(cars.describe())

"""## 1. Find the car with the highest MPG"""

print(f"Car with highest MPG: {cars[cars.MPG == cars.MPG.max()]}")

"""## 2. Find the average MPG per Cylinder count"""

print(f"Average MPG per Cylinder count: {cars.groupby('Cylinders').MPG.mean()}")

"""### 3. Find each make's average MPG (Cherry, Ford, etc)"""

# Extracting Make of the Car from the Car name
cars['Make'] = cars.Car.str.split(' ').str[0]

# Replacing misspelled Make
misspelled = {'Chevrolete': 'Chevrolet',
              'Hi': 'Harvester',
              'Mercedes-Benz': 'Mercedes',
              }
cars['Make'] = cars['Make'].replace(misspelled)

print(f"Average MPG by Make: {cars.groupby('Make').MPG.mean()}")

"""### 4. Build a predictive model to predict the MPG based on other data. What is your optimal error metric on the test set?"""

cars.head()

plt.figure(figsize=(10,10))
sns.heatmap(cars.corr(), annot=True)

cars.head()

"""#### Target (MPG)"""

sns.set_style('darkgrid')
plt.figure(figsize = (8, 6))
sns.kdeplot(cars.MPG)
plt.title("KDE plot of MPG")
plt.show()

cars['MPG'] = np.log1p(cars.MPG)

sns.set_style('darkgrid')
plt.figure(figsize = (8, 6))
sns.kdeplot(cars.MPG)
plt.title("KDE plot of MPG After log1p transformation")
plt.show()

"""### One Hot Encoding the feature columns"""

cars = pd.get_dummies(cars, columns=['Cylinders','Origin','Make'], drop_first=True)

"""#### Train test split"""

X = cars.drop(['MPG', 'Car'], axis = 1)
y = cars['MPG']
X_train, X_test, y_train, y_test = train_test_split(X , y,test_size  = 0.2, random_state = 116)

print(f'X_train Shape: {X_train.shape}\nX_test Shape: {X_test.shape}')
print(f'y_train Shape: {y_train.shape}\ny_test Shape: {y_test.shape}')

ss_train = StandardScaler()
ss_test = StandardScaler()

X_train = ss_train.fit_transform(X_train)
X_test = ss_test.fit_transform(X_test)

"""### Predictive Models

#### XGBRegressor
"""

# Using grid search to find the optimal hyper parameters
param_dict = {'n_estimators':[100,300,500,800], 'max_depth':[2,4,6,8]}
gs = GridSearchCV(XGBRegressor(), param_grid=param_dict, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=6, refit=True)
gs.fit(X_train, y_train)
xgb = gs.best_estimator_
print(f"Best parameters are {gs.best_params_}")
print(f"Best score is {gs.best_score_}")

xgb_predict_train = xgb.predict(X_train)
xgb_predict_test = xgb.predict(X_test)
print(f"RMSE for train is {np.sqrt(mean_squared_error(y_train, xgb_predict_train))}")
print(f"RMSE for test is {np.sqrt(mean_squared_error(y_test, xgb_predict_test))}")
print(f"R2 score for train is {r2_score(y_train, xgb_predict_train)}")
print(f"R2 score for test is {r2_score(y_test, xgb_predict_test)}")

"""#### AdaBoostRegressor"""

# Using grid search to find the optimal hyper parameters
param_dict = {'n_estimators':[100,300,500,800], 'learning_rate':[1, 10, 100]}
gs = GridSearchCV(AdaBoostRegressor(), param_grid=param_dict, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=6, refit=True)
gs.fit(X_train, y_train)
adb = gs.best_estimator_
print(f"Best parameters are {gs.best_params_}")
print(f"Best score is {gs.best_score_}")

adb_predict_train = adb.predict(X_train)
adb_predict_test = adb.predict(X_test)
print(f"RMSE for train is {np.sqrt(mean_squared_error(y_train, adb_predict_train))}")
print(f"RMSE for test is {np.sqrt(mean_squared_error(y_test, adb_predict_test))}")
print(f"R2 score for train is {r2_score(y_train, adb_predict_train)}")
print(f"R2 score for test is {r2_score(y_test, adb_predict_test)}")

"""#### RandomForestRegressor"""

# Using grid search to find the optimal hyper parameters
param_dict = {'n_estimators':[100,300,500,800], 'max_depth':[2,4,6,8]}
gs = GridSearchCV(RandomForestRegressor(), param_grid=param_dict, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=6, refit=True)
gs.fit(X_train, y_train)
rf = gs.best_estimator_
print(f"Best parameters are {gs.best_params_}")
print(f"Best score is {gs.best_score_}")

rf_predict_train = rf.predict(X_train)
rf_predict_test = rf.predict(X_test)
print(f"RMSE for train is {np.sqrt(mean_squared_error(y_train, rf_predict_train))}")
print(f"RMSE for test is {np.sqrt(mean_squared_error(y_test, rf_predict_test))}")
print(f"R2 score for train is {r2_score(y_train, rf_predict_train)}")
print(f"R2 score for test is {r2_score(y_test, rf_predict_test)}")

print(
"""
XGBRegressor model has the best performance on the test set.

Root Mean Square Error for test set: 0.10003268892242662 (lower is better)

R2 score for the test set: 0.8971481122614369 (higher is better)
"""
)

"""XGBRegressor model has the best performance on the test set.

Root Mean Square Error for test set: 0.10003268892242662 (lower is better)

R2 score for the test set: 0.8971481122614369 (higher is better)
"""

!pip freeze